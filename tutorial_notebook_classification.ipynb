{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necesseary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\peds\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from datamodule import TextDataModule\n",
    "from pl_model import ModelLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loader\n",
    "\n",
    "## TextDataModule\n",
    "\n",
    "The `TextDataModule` is a custom PyTorch Lightning data module responsible for handling the dataset and tokenizer used in your project. It helps with efficient data loading, processing, and batching. This module is part of the workflow to streamline handling data for model training, validation, and testing.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `config_path`: This parameter points to a JSON configuration file for your dataset. It is used to load specific settings related to data paths, preprocessing steps, and other configurations necessary to set up the dataset.\n",
    "  - Example: `./configs/dataset_config.json`\n",
    "  \n",
    "- `tokenizer_path`: Path to a tokenizer file, which is necessary for converting text data into a suitable format for model consumption. In this case, the tokenizer is character-level, stored as a JSON file.\n",
    "  - Example: `character_level_tokenizer.json`\n",
    "  \n",
    "- `batch_size`: The size of each data batch to be fed into the model during training. This can be customized depending on memory constraints or the size of your dataset.\n",
    "  - Example: `512`\n",
    "\n",
    "### Methods\n",
    "\n",
    "- **`setup()`**: This method prepares the data for use. It is called to load the dataset, apply necessary preprocessing, and tokenize the data. This is typically run once during initialization or before training begins.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "Here’s an example of how you can instantiate and use the `TextDataModule`:\n",
    "\n",
    "```python\n",
    "# Initialize the data module with a dataset configuration, tokenizer, and batch size\n",
    "data_module = TextDataModule(\n",
    "    config_path=\"./configs/dataset_config.json\",  # Path to dataset config\n",
    "    tokenizer_path='character_level_tokenizer.json',  # Path to tokenizer\n",
    "    batch_size=512  # Set batch size\n",
    ")\n",
    "\n",
    "# Setup the data module (e.g., load and process dataset)\n",
    "data_module.setup()\n",
    "\n",
    "# You can now access data loaders using:\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = TextDataModule(\n",
    "    config_path=\"./configs/dataset_config.json\",  # Path to your dataset\n",
    "    tokenizer_path='character_level_tokenizer.json',  # Path to tokenizer\n",
    "    batch_size=512  # Customize batch size if needed\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_module.tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BI LSTM model\n",
    "\n",
    "## Running the ModelLSTM with PyTorch Lightning\n",
    "\n",
    "This section describes how to set up and train the `ModelLSTM` using PyTorch Lightning. We will define the model, set up callbacks, initialize the trainer, and then train the model.\n",
    "\n",
    "### Step 1: Model Initialization\n",
    "\n",
    "We initialize the `ModelLSTM` with the following parameters:\n",
    "- `in_dim`: The input dimension, which is 40 in this case (typically the feature size of each sequence element).\n",
    "- `embedding_dim`: The size of the embedding layer. Here, it is set to 64.\n",
    "- `hidden_dim`: The number of hidden units in the LSTM. Set to 64.\n",
    "- `out_dim`: The output dimension. For binary classification, this is 1.\n",
    "- `max_len`: The maximum sequence length, retrieved from the data module.\n",
    "- `lr`: The learning rate, set to 0.002.\n",
    "\n",
    "```python\n",
    "model = ModelLSTM(\n",
    "    in_dim=40,\n",
    "    embedding_dim=64,  # Embedding dimension\n",
    "    hidden_dim=64,  # Hidden dimension for LSTM\n",
    "    out_dim=1,  # Output dimension - Number of target classes\n",
    "    max_len=data_module.max_len[0],  # Max sequence length from the data module\n",
    "    lr=0.002,  # Learning rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\anaconda\\envs\\peds\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "d:\\anaconda\\envs\\peds\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\arekp\\OneDrive\\Desktop\\antibody_task\\models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | nn               | Bi_LSTM               | 649 K  | train\n",
      "1 | loss_fn          | BCELoss               | 0      | train\n",
      "2 | train_acc        | BinaryAccuracy        | 0      | train\n",
      "3 | val_acc          | BinaryAccuracy        | 0      | train\n",
      "4 | train_f1         | BinaryF1Score         | 0      | train\n",
      "5 | val_f1           | BinaryF1Score         | 0      | train\n",
      "6 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "649 K     Trainable params\n",
      "0         Non-trainable params\n",
      "649 K     Total params\n",
      "2.600     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\peds\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.73it/s]Confusion Matrix:\n",
      "tensor([[  0, 512],\n",
      "        [  0, 512]], device='cuda:0')\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\peds\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1381/1381 [02:13<00:00, 10.37it/s, v_num=43, train_loss=5.99e-5, train_acc=1.000, train_f1=1.000] Confusion Matrix:\n",
      "tensor([[74982,    14],\n",
      "        [    1, 76514]], device='cuda:0')\n",
      "Epoch 1:  11%|█▏        | 158/1381 [00:18<02:21,  8.62it/s, v_num=43, train_loss=2.84e-5, train_acc=1.000, train_f1=1.000, val_loss=0.000281, val_acc=1.000, val_f1=1.000] "
     ]
    }
   ],
   "source": [
    "model = ModelLSTM(\n",
    "    in_dim=input_dim,\n",
    "    embedding_dim=64,  # Embedding dimension\n",
    "    hidden_dim=64,  # Hidden dimension for LSTM\n",
    "    out_dim=1,  # Output dimension - Number of target\n",
    "    max_len=data_module.max_len[0],\n",
    "    lr=0.002,\n",
    ")\n",
    "\n",
    "# Step 2.5: Create callbacks\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./models\", save_top_k=1, monitor=\"val_loss\", filename=\"model\")\n",
    "\n",
    "# Step 3: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,  # Number of epochs\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "# Step 4: Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = ModelLSTM.load_from_checkpoint(\"models/model.ckpt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "tensor([[74988,     9],\n",
      "        [    1, 76514]])\n",
      "Test Accuracy: 0.9999\n",
      "Test Precision: 0.9999\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "# Initialize metrics for binary classification\n",
    "accuracy_metric = torchmetrics.Accuracy(task=\"binary\")\n",
    "precision_metric = torchmetrics.Precision(task=\"binary\")\n",
    "recall_metric = torchmetrics.Recall(task=\"binary\")\n",
    "f1_metric = torchmetrics.F1Score(task=\"binary\")\n",
    "confusion_matrix = torchmetrics.ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "# Assuming you already have your test dataloader\n",
    "test_dataloader = data_module.test_dataloader()\n",
    "\n",
    "# Iterate over the test dataset and calculate metrics\n",
    "for batch in test_dataloader:\n",
    "    # Get the inputs and labels\n",
    "    labels = batch['label']\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    preds = trainer.model.predict_step(batch)\n",
    "    preds = (preds > 0.5).long().flatten()\n",
    "    \n",
    "    # Convert predictions to class indices (if your model outputs probabilities or logits)\n",
    "    \n",
    "    # Update metrics with predictions and true labels\n",
    "    accuracy_metric.update(preds, labels)\n",
    "    precision_metric.update(preds, labels)\n",
    "    recall_metric.update(preds, labels)\n",
    "    f1_metric.update(preds, labels)\n",
    "    confusion_matrix.update(preds, labels)\n",
    "\n",
    "# Compute final metrics\n",
    "accuracy = accuracy_metric.compute()\n",
    "precision = precision_metric.compute()\n",
    "recall = recall_metric.compute()\n",
    "f1 = f1_metric.compute()\n",
    "\n",
    "# Print the results\n",
    "cm = confusion_matrix.compute()\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Test Accuracy: {accuracy.item():.4f}\")\n",
    "print(f\"Test Precision: {precision.item():.4f}\")\n",
    "print(f\"Test Recall: {recall.item():.4f}\")\n",
    "print(f\"Test F1 Score: {f1.item():.4f}\")\n",
    "\n",
    "# Reset metrics for potential future use\n",
    "accuracy_metric.reset()\n",
    "precision_metric.reset()\n",
    "recall_metric.reset()\n",
    "f1_metric.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
